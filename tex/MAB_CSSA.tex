\graphicspath{ {img/MAB_CSSA/} }
\chapter[A superprocess with Upper Confidence Bounds for Cooperative Spectrum Sharing][A superprocess with UCB for CSS]{A superprocess with Upper Confidence Bounds for Cooperative Spectrum Sharing}\label{MAB_CSSA_chap}

\section{Introduction}
The mechanism we proposed in the previous chapter was appropiate when the spectrum owner has unused spectrum to lease. What happens if the PU demands are higher and there are not so many opportunities to sell? Is automated spectrum trading possible in such context? In this chapter we give a positive answer to that question, in the framework of \textit{Cooperative Spectrum Sharing} (CSS), also known as \textit{Cooperative Secondary Spectrum Access} (CSSA). 
More specifically, we are interested in the trading scenario introduced by \cite{ref:Simeone2008}. 
The basic premise of CSS is that SUs without license may act as transmission relays for a PU in exchange for transmission opportunities in the spectral resources of the PU.

CSS is different from other market-driven spectrum sharing approaches in that it fosters the creation of transmission opportunities in the PU spectrum in an exogenous way: by increasing the transmission rate of the PU, it reduces its spectrum usage. 
Because of that, CSS is particularly useful when the PU's own demands are so high that it would rarely have spectrum to lease.
\footnote{It could be the case that the PU has a low traffic volume and it is not interested in increasing its transmission rate. 
For that reason, hybrid approaches combining CSS and other DSA ideas, with monetary transactions, have also been studied \cite{ref:Zhang2009,ref:Zhang2012_Fair}.}

As we highlighted in chapter \ref{Survey_chap}, most previous works are focused on developing complex algorithms which may not be fast enough for real-time decisions and/or assume perfect information about the network. Instead, we develop a learning mechanism for a PU to enable CSS in a strongly incomplete information scenario, with low computation overhead. Our mechanism is based on a Markovian variant of multi-armed bandits (MABs) called superprocess, enhanced with the concept of Upper Confidence Bound (UCB) from stochastic MABs. By means of Monte-Carlo simulations we show that, despite its low computational overhead, it outperforms baseline approaches such as $\epsilon$-greedy.

\subsection{Motivation}
A CSS system poses the following \textit{key challenges}: 1) spectrum resource allocation; 2) the SUs may belong to self-interested networks different from the PU's, and thus, the PU should not expect the SUs to collaborate in maximizing the PU's profit; 3) the PU has to undergo a negotiation process with the nearby SUs, having no previous information about them, in general; 4) spectrum opportunities may happen on a short timescale (of the order of seconds or less), thus, for CSS to be effective, this negotiation must be carried out in real-time; 5) the situation around the PU can change quickly (\textit{e.g.,} SUs arriving and leaving the system). 

Previous works in CSS have not addressed these issues simultaneously. As we discussed in \cite{ref:Mario2014} and we explain in Section \ref{sec:Rel}, they are focused on requirements 1-3, but the time needed to reach elaborated solutions (requirements 4 and 5) is not considered in detail. 
Multiple factors in spectrum trading (\textit{e.g.,} supply, demand, channel gains) vary rapidly with time, and trying to reach a complex allocation solution for a particular spectrum opportunity could take so long that it may render the solution impractical.

We focus on meeting all the aforementioned requirements from the perspective of a PU willing to transmit (a primary transmitter or PT), requesting help from SUs to communicate with an intended primary receiver (PR). The PT has to gradually learn the optimal SU - offer combination based on its accumulated observations. Our proposal aims to achieve a balance between the time devoted by the PT to exploration of options and the time spent exploiting the best alternative known so far, to maximize the PT's payoff (in terms of net transmitted data) over time.
\subsection{Related Work}\label{sec:Rel}
Many previous works in CSS such as \cite{ref:Simeone2008,ref:Zhang2009,ref:Yi2010,ref:Nadkar2011} consider perfect information scenarios. 
They assume global knowledge of the network, actions and payoffs of other entities present in the system, and even private information like the value an entity gives to the traded good.
It is unlikely that the SUs belonging to a different operator would reveal private information to the PT, taking into account that they may obtain higher payoffs by hiding it or even lying, at the cost of worsening the whole system. 
Some of this private information, in addition, is non-measurable. For example, if a PT suspects an SU is lying about its transmission power, it could check what the transmission power of an SU is, but there is no way to know how much battery it has left.
The incompleteness of information in our scenario is stronger than in most works addressing this issue. In \cite{ref:Duan2014}, the probability distribution of SU-PR average channel gains is fully known, and \cite{ref:Feng2014} assumes that the PT knows the set of such averages for the SUs present in its coverage area. Instead, we completely rely on learning to discover this information, although prior belief distributions can be easily included in our mechanism. 

Other approaches like the ones proposed in \cite{ref:Yuan2013,ref:Han2010,ref:Li2011} consider cooperation between networks belonging to different operators. 
Although it is possible to design mechanisms that provide incentives to selfish SUs to collaborate with the PT (cooperation in terms of game theory \cite{ref:Zhang2012_Fair}), they require the exchange of several messages between these individuals, and therefore a loss in transmission efficiency, as pointed out in \cite{ref:Niyato2008}.

These selfish entities, however, do have incentives to make strategies and/or collude against the PT. 
In our proposal, the PT employs one-to-one bargaining instead of broadcast offers, as \cite{ref:Yan2013}, but we consider more than one SU. The benefits of one-to-one transactions versus broadcast mechanisms \cite{ref:Feng2014,ref:Duan2014,ref:Jayaweera2011,ref:Zhang2009,ref:Simeone2008} are: the reduction of the strategic power of the SUs, as they cannot overhear public offers or other information about their competitors; their robustness against collusions of SUs \cite{ref:Alcaraz2014_coa}; and the reduction of the communication overhead on the control channels in comparison to widespread mechanisms such as auctions \cite{ref:Feng2014, ref:Jayaweera2011} (\textit{e.g.,} multiple rounds of bidding messages).
 
Auction mechanisms are also a popular approach to handle the incomplete information issues in CSS.
Truthful auctions allow optimal solutions without assuming the knowledge of private information, but at the cost of complex winner-selection algorithms and/or increased communication overheads. 

\begin{landscape}
\begin{table*}
\centering
\begin{threeparttable}
\rowcolors{1}{}{lightgray}
\caption{Summary of related works}
\label{MAB_CSSA_table_ref}
\begin{tabular}{p{2.7cm}*{7}{p{2.5cm}}}

\hline
    & \textbf{Our work} & \cite{ref:Feng2014} & \cite{ref:Duan2014} & \cite{ref:Yan2013} & \cite{ref:Zhang2012_Fair} & \cite{ref:Yi2010} &  \cite{ref:Yuan2013} \\\hline

Information & \textbf{Strongly \mbox{incomplete}} & Incomplete & Incomplete & \textbf{Strongly \mbox{incomplete}} & Complete & Complete & Complete \\
Learning & \textbf{Exploration - exploitation} & Greedy search & No & \textbf{Repeated game} & N/A & N/A & N/A \\
Penalization for rejected offers & \textbf{Yes} & No & N/A & \textbf{Yes} & N/A & N/A & N/A \\
Communication & \textbf{Unicast} & Broadcast & Broadcast & \textbf{Unicast} & Broadcast & Broadcast & Broadcast \\
Technique & \textbf{\mbox{MAB -} \mbox{superprocess}} & Matching Theory & Contract \mbox{Theory} & Bargaining & Co-op. \mbox{bargaining} & Stackelberg game  & MILP \mbox{optimization}\\
Market form & \textbf{1 PU - n SUs} & \textbf{1 PN - n SUs} & \textbf{1 PU - n SUs} & 1 PU - 1 SU & \textbf{1 PU - n SUs} & 1 PN - 1 SN & n PUs - n SUs\\
Objective & \textbf{\mbox{Max. PU} \mbox{reward}} & \textbf{\mbox{Max. PU} \mbox{reward}} & \textbf{\mbox{Max. PU} \mbox{reward}} & Equilibrium (bias towards SUs) & \mbox{Max. SU} rewards and fairness & Equilibrium (bias towards PU) & Configurable \\
Year & \textbf{2014} & \textbf{2014} & \textbf{2014} & 2013 & 2012 & 2010 & 2013\\\hline
\end{tabular}
\begin{tablenotes}
\item \hspace{1em} N/A = Not applicable, PN = Primary Network, SN = Secondary Network, MILP = Mixed Integer Linear Programming
\end{tablenotes}
\end{threeparttable}
\end{table*}
\end{landscape}
This may not be suitable for spectrum trading in short timescales. There are other mechanisms such as the stochastic optimization based on contract theory proposed in \cite{ref:Duan2014}, in which the authors do not assume perfect information and are concerned about the complexity of the algorithm. Nevertheless, in contrast to our proposal, they do not implement any learning process from successive interactions with SUs. Independent learning is a key feature of self-organizing networks and scalability, as indicated in the survey on market mechanisms for CSS with incomplete network information in \cite{ref:Huang2013}.

Our work is inspired by \cite{ref:Yan2013}, where the authors consider CSS between a PU and a cognitive SU pair in a repeated bargaining game. 
The bargaining partner selection problem in our work is not comparable to the one formulated in \cite{ref:Calvo2002} because we do not assume knowledge about the impatience of other players, and we consider the PU has full bargaining power and learning abilities, among other differences. 
Bargaining partner selection is also different from traditional best-relay selection problems in multi-hop networks or cooperative networks \cite{ref:Yuan2013,ref:Tran2014}, which expect an internetwork collaborative behavior in the form of relays revealing their true link qualities, battery life, etc.

Our research in this chapter is built on the techniques for multi-armed bandit with dependent arms developed by D.B. Brown and J.E. Smith in \cite{ref:Brown2013} and S. Pandey, D. Chakrabarti and D. Agarwal in \cite{ref:Pandey2007}. In \cite{ref:Brown2013}, a MAB superprocess is used to develop an optimal policy for exploring oil and gas fields in the North Sea. The state space of their problem is finite, unlike ours. 
Our dependency model is also different enough from \cite{ref:Pandey2007} to require substantial changes in their algorithm, such as the integration with an MDP. 
Multi-armed bandits have been previously used in spectrum sharing, mainly for sensing or resource allocation \cite{ref:Si2010}. 
Surveys on bandits and other machine learning algorithms in cognitive radio can be found in \cite{ref:Bkassiny2013} and \cite{ref:Gavrilovska2013}.
To the best of our knowledge, however, this it the first work exploiting MABs in CSS. 

Other related works explore alternative flavors of CSS. For example \cite{ref:Tran2014} explores the possibility of both PUs and SUs acting as relays of each other assuming a cooperative attitude between them. 
The work in \cite{ref:Shao2014} shows the optimal strategy of an SU which is allowed to dynamically decide whether it enters a CSSs or a spectrum leasing market. A closely related topic to CSS is the overlay cognitive radio network paradigm \cite{ref:Goldsmith2009}, by which an SU acts as relay for the PU and, at the same time, transmits its own message. 
The interference constraint on the SU transmission is enlarged thanks to the cooperative transmission and the use of interference cancellation techniques \cite{ref:Han2010}. 

Table \ref{MAB_CSSA_table_ref} summarizes the most significant features of our work and compares it to previous works. 
Note that for those works analyzing different configurations, we select and compare to the one closest to our proposal. 

\subsection{Our Contribution}

\textit{We develop a new approach for the PT learning problem in the context of CSS}, by means of a stochastic multi-armed bandit (MAB) problem with dependent arms. This approach allows the PT to group the information gathered from SUs instead of treating every possible action independently. Thus, the PT learns in a more efficient way from its observations compared to other reinforcement learning approaches. We formulate the main problem in Section \ref{sec:Sys}.
The reward function of our MAB comprises two factors: one is drawn from a probability distribution, and the other one is determined by a Markov chain.
To the best of our knowledge, there are no algorithms in the previous literature for this type of rewards in MABs. 
In Section \ref{sec:MAB}, \textit{we describe our MAB-MDP algorithm}, in which we integrate a stochastic MAB and a Markov Decision Process (MDP). MAB-MDP allows us to introduce some key concepts used in Section \ref{sec:Super} to build \textit{a new algorithm for solving the stochastic MAB with dependent arms. We call this algorithm Superprocess with Upper Confidence Bounds (Super-UCB)}, because it combines the classic UCB indices used in stochastic MABs and an extension to Markovian MABs \cite{ref:Auer2002} called superprocess. \textit{Super-UCB makes a more efficient use of the available information, achieving a better performance without increasing the computational overhead.} \textit{We evaluate the performance, scalability and robustness of Super-UCB with Monte Carlo simulations} in Section \ref{sec:Num}.
Our solution is not only directly implementable, without strong assumptions, but also extensible to more complex scenarios, as summarized in the conclusions in Section \ref{sec:Con}.

% All agents are considered to be self-interested, in that they look for their own profit only. 
% In each transmission period, the PT has to choose an SU to act as a relay, and make this SU an offer in terms of SU transmission time.
% The PT obtains a payoff in terms of net transmitted data.
% The average path loss of each SU-PR channel are initially unknown by the PT. 
% The PT also ignores the offers that each SU would be willing to accept. 
% Therefore it has to gradually learn the optimal SU - offer combination based on the accumulated observations. 
% Because in a realistic setting the situation around the PT can change quickly (\textit{e.g.,} SUs arriving and leaving the system), our proposals aim to achieve a balance between the time devoted by the PT to exploration of options and the time spent exploiting the best alternative known so far, to maximize the PT's payoff over time.

\section{System Model}\label{sec:Sys}

\begin{figure}[!t]
\centering
\includegraphics{geo.eps}
\caption{Cooperative Spectrum Sharing.}
\label{fig:geo}
\end{figure}

The protocol considers a PU transmitter (PT) - receiver (PR) pair and a set of SU cognitive pairs in the coverage area of the PT denoted by $\mathcal{S}\equiv\left\{s_1,s_2,\ldots,s_S\right\}$, as in Fig. \ref{fig:geo}.
The system is under the ``exclusive-use'' coexistence model \cite{ref:Zhao2007_sur}, by which the PUs are the only entities with the right to transmit in a certain band. 
When the PU pair's channel condition is not suitable for direct transmission, the PT would be willing to use the SUs as relays.

In exchange for its services, the PT makes an offer to the SU, consisting of a certain amount of time for SU data transmission over the PT channels. 
The SUs transmit with fixed power, the same for relaying and their own transmissions. 
%The SUs are interested in extra transmission opportunities but they are assumed to have their own resources.
The SUs are assumed to have their own, but limited, spectral resources. Therefore, although it is not crucial for their communication purposes, the SUs may benefit from the additional spectrum resources obtained from the PT.
As a consequence, it is the PT who contacts the SU and initiates the bargaining.

Time is divided into fixed transmission periods or \textit{frames}, which we will consider of duration $T = 1$ units of time and numbered as $n=1,2,\ldots$. 
The offer that the PT makes, denoted by $\alpha \in [0,1]$, is the fraction of the transmission period during which the SU is allowed to transmit its own data.
For tractability, we discretize that interval in equal increments and $\mathcal{A} = \left\{\alpha_{1},\ldots,\alpha_{A}\right\}$ denotes the set of possible offers to make to SU $s \in \mathcal{S}$.\footnote{In practice we will assume $\alpha_{A} = 0.9$, and $\alpha_1>0$, as neither the SU nor the PT may ``work for free''. Also note that our model could consider different offer sets $\mathcal{A}_s$ for each SU.} 

Each transmission period is composed of a decision phase, an optional bargaining phase, and a communication phase, as in \cite{ref:Simeone2008,ref:Yan2013}. See also Fig. \ref{fig:phase}. 

\begin{figure}[!t]
\centering
\includegraphics{phase.eps}
\caption{Frame structures.}
\label{fig:phase}
\end{figure}

\begin{itemize}
\item Decision phase. During this phase, the PT has to choose which SU to bargain with and what offer to make to it: $(s,\alpha)\in\mathcal{S}\times\mathcal{A}$. The PT aims to maximize its accumulated payoff, and its decisions are based on previous interactions.
\item Bargaining phase. We consider the PT has full bargaining power and makes a take-it-or-leave-it offer to an SU. This phase models the time spent in sending that message and waiting for the answer. 
\item Communication phases. If the offer is rejected by the SU, the PT directly transmits for the rest of the frame to its intended receiver. 
If the offer is accepted by the SU, then the communication phase is divided into three time periods: 
\begin{itemize}
\item Direct transmission of the PT. The PT transmits its information to its receiver and to the selected SU.
\item SU forwarding. The selected SU re-transmits the information to the PR.
\item SU's own transmission.
\end{itemize}
\end{itemize}

\subsection {Payoff Functions}

\textbf{PT's payoff.} For the cooperative transmission, the system employs the decode and forward relay communication scheme from \cite{ref:Laneman2001}. 
For a given SU $s\in\mathcal{S}$, the links initially involved are: PT-PR, PT-$s$, and $s$-PR. Let $\Gamma_{\text{P,P}}^{(n)}$, $\Gamma_{\text{P,}s}^{(n)}$, and $\Gamma_{s}^{(n)}$ be their respective SNR values averaged over the duration of transmission frame $n$.
The achievable data rate satisfies
$R_{\text{P}}^{(n)} = K\min\{\log(1+\Gamma_{\text{P,}s}^{(n)}),\log(1+\Gamma_{\text{P,P}}^{(n)}+\Gamma_{s}^{(n)})\}$, where $K$ is a constant.
We assume that the SUs always decode the PT data correctly in the first phase, and thus we focus on the $s$-PR link and 
%This assumption is justified by the fact that, as noticed in \cite{ref:Duan2014}, the SUs involved in relaying the signal will probably be the ones closer to the PU transmitter\footnote{Because they are the ones which can ``hear'' the request of the PT.}, and the SU-PR channel becomes the bottleneck
%\footnote{It is worth noting that the period of time of direct transmission and the period of SU acting as relays should not be necessarily equal, as this would depend on the achievable data rates of those links and could be optimized, or even learned in an online manner.}. 
we have 
$R_{\text{P}}^{(n)} = K\log(1+\Gamma_{\text{P,P}}^{(n)}+\Gamma_{s}^{(n)})$.
Since the PT-PR and s-PR links are considered to be in bad and good propagation conditions respectively, \textit{i.e.,} $\Gamma_{\text{P,P}}^{(n)}\ll\Gamma_{s}^{(n)}$, and $1 \ll\Gamma_{s}^{(n)}\text{ }\forall n$, the achievable data rate can be approximated as
$R_{\text{P}}^{(n)} \approx K\log(\Gamma_{s}^{(n)})$.

The time allocated to the $s$-PR transmission within a frame can be considered long compared to fast fading variations. That is, the effect of multipath is assumed to be negligible in terms of the average SNR, $\Gamma_s^{(n)}$. Therefore, $\Gamma_s^{(n)}$ is mainly determined by pathloss attenuation and shadowing, remaining constant during an $s$-PR transmission. Successive SU relay phases between $s$ and the PR are sufficiently distant in time for the channel to decorrelate. Thus, the SNR samples $\Gamma_s^{(n)}$ are modeled as i.i.d. random variables, following a log-normal distribution which typically characterizes shadowing \cite{ref:Goldsmith2009}, \textit{i.e.,} $\log(\Gamma_s^{(n)})=\gamma_s^{(n)} \sim N(\mu_{\gamma_s},\sigma_{\gamma_s})$. 

%\footnote{Another interesting approach could be imposing a target data rate transmission to the SU relay, without CSI, and not assuming that $\gamma_{s}$ can be observed, but just if a packet arrived correctly or incorrectly (outage). The problem would be learning the outage probabilities of each SU for that target rate, and possibly studied with a Bernoulli-MAB. In such scenario, the variances of the SNRs would have a greater impact.}%
The PR can observe the value of $\gamma_s^{(n)}$ and feed it back to the PT at the end of each transmission frame. Then, the reward for the PT in frame $n$ is the transmission rate it gets, multiplied by the time that transmission lasts:
\begin{equation}\label{eq:W}
W^{(n)}(s,\alpha) = \left\{ 
  \begin{array}{l l}
    (1-\beta)(1-\alpha)\log({\Gamma_s^{(n)}}) & \text{  } \text{if } \alpha \text{ is accepted}\\
    (1-\beta)\log(1+\Gamma_{\text{P,P}}^{(n)}) & \text{  } \text{otherwise}
  \end{array} \right.
\end{equation}
For the sake of clarity, from now on we set $\beta=0$ and $\log{(1+\Gamma_{\text{P,P}}^{(n)})} = 0$ for all $n$. Nevertheless, our model is also applicable with different values of these parameters. 

%We can define the utility function of the PT as $U_p(u^{(1)},\ldots,u^{(n)}) = \mathbb{E}\left[\sum_{k=1}^{n} W(u^k)\right]$, for an unknown, finite time horizon $n$.

%An SU, $s$, can be aware of actual channel gain at the $s$-PR link (for example, using a pilot signal) but does not disclose this information to the PT. %The outage probability of its transmission is negligible. If the channel is in deep fading during a frame, the achieved data rate will be low.
%In Section \ref{sec:Det}, that measure is the true average received SNR (for example, an accurate estimation of the SU). In Section \ref{sec:Sto}, that measure will be a sample drawn from the log-normal distribution characterizing the received SNR from that SU.

\textbf{SU's payoff.} The payoff obtained by an SU acting as relay is the difference between its net transmitted data during a frame when using the PT channel, $\alpha R_{s}^{\text{P}}$, and the net data it would transmit when using its own spectral resources $R_{s}$. An SU accepts any offer that provides positive payoff, which results in a threshold behavior. 
SU $s$ accepts an offer $\alpha_a \in \mathcal{A}$ whenever $\alpha_a \geq \alpha_s^*$, where $\alpha_s^*$ is the minimum offer SU s is willing to accept. The \textit{type of an SU}, $\tau_s$, is the index of the smallest offer that this SU accepts, i.e. $\tau_s = \argmin_a\{\alpha_a: \alpha_a\geq \alpha_s^*, \alpha_a \in \mathcal{A}\}$.
If the link between the SU pair is stable (\textit{e.g.,} a close ad hoc connection) and the offered PT bandwidth is constant (only the time offered changes), the thresholds, and therefore the type of each SU, remain constant over multiple transmission frames.

\subsection {Multi-armed bandit formulation}
Mathematically, we model the sequential decisions of the PT as a multi-armed bandit (MAB) problem.
The PT selects SU-offer pairs $(s,\alpha)$ from the action set $\mathcal{U}=\mathcal{S}\times\mathcal{A}$.
In the MAB model each $u = (s,\alpha)$ is an arm, and $\mathcal{U}$ is the set of arms.
%For a given $u=(s,\alpha)\in\mathcal{U}$, 
%At each decision round $n$, the PT selects one SU-offer pair $u^{(n)} = (s^{(n)},\alpha^{(n)})\in\mathcal{A}$.
%In the MAB model, each $u = (s,\alpha)$ pair is an arm, and $\mathcal{U}=\mathcal{A}$ is the set of arms.
%Let us denote the set of SUs present in the system as $\mathcal{S}$, and the set of possible offers as $\mathcal{A}$. 
%Our MAB problem consists of $|I|$ arms, where the set of arms $\mathcal{U}$ consists of the cartesian product of SUs and possible offers: $\mathcal{U} = \mathcal{S} \times \mathcal{A}$.
%Upon the start of each frame $n = 1,2,\ldots$, the PU chooses to pull one of the arms (pick an SU and an offer) and receives a reward $W$, that equals the utility of the PU:
At round $n$, the arm pulled is $u^{(n)} = (s^{(n)},\alpha^{(n)})$, and the reward received by the PT is $W^{(n)} = W^{(n)}(u^{(n)})$.
The history of the system up to time $n$ is defined as the sequence of decisions and observed rewards $W^{(0)},u^{(1)}, W^{(1)},\ldots u^{(n)}, W^{(n)}$ (where $W^{(0)}$ corresponds to the initial samples $\gamma_s^{(0)}$).
A \textit{policy} $\pi$ is a function that, at each round $n$, prescribes a decision $u^{(n+1)}$ based on the history of the system. Therefore $\pi$ induces a history $W^{(0)},u_\pi^{(1)},W_\pi^{(1)},u_\pi^{(2)},W_\pi^{(2)},\ldots$. 
The usual performance metric in learning problems is the \textit{regret} \cite{ref:Bubeck2012}. 
At decision stage $n$, we define the regret of a policy $\pi$ by $r^{(n)} = \max_{u}\mathbb{E}\left[\sum_{k=1}^{n}\left(W^{(k)}(u) - W^{(k)}_{\pi}\right)\right]$.
The regret quantifies the performance loss of a learning policy with respect to a policy that makes the best decision on average.
In a realistic setting, the PT does not know such best decision, and faces the challenge of learning it while trying to maximize the reward. 
This is known as the \textit{exploration - exploitation} tradeoff, a term coined in \cite{ref:Gittins1974} and extensively used afterwards in the related literature \cite{ref:Sutton1998,ref:Powell2012}. It describes the problem of finding the optimal balance between getting immediate rewards and gathering additional information to make better decisions in future rounds. In MABs, this is the balance between pulling the arms that seem to be better in expectation and those that may seem to be worse initially but could potentially be the best.
%It is worth saying that the expectation taken on $W^k_{\pi}$ includes three random processes taking place: 1) the random values of the rewards, 2) the different decisions of a policy given the different observations and 3) randomized policies (such as the $\epsilon$-greedy policy we use as a comparison in Section \ref{sec:Num}).

\textbf{Correlated arms.} Learning about an arm of the MAB should not be treated independently of the other arms. The reward obtained from an SU $s$ when offered a particular $\alpha$ provides information about the rewards of all the other offers (the other arms of the same SU). Before making an offer $\alpha$ to an SU $s$, the PT holds a prior belief about the acceptance probability of each offer by $s$.  The acceptance or rejection of an offer implies an update of this belief. In addition, when the PT observes a sample of $\gamma_s$ and updates its sample mean $\overline\gamma_s^{(n)}$, that affects the belief about the reward of all the arms $\{(s,\alpha),\text{ } \forall \alpha \in \mathcal{A}\}$ of that SU $s$.

In the next sections we explain how, inspired by \cite{ref:Pandey2007} and \cite{ref:Brown2013}, respectively, we handle correlation by grouping the observations of an SU, exploiting mutual information. 
\textit{We reduce the original MAB} with $|\mathcal{U}| = S \times A$ arms to a MAB with $S$ arms, one for each SU, where each arm integrates the information the PT has observed about each SU up to round $n$, that is, the acceptance and rejection of offers, and the SNR samples $\gamma_s^{(n)}$. We must then answer two questions: \textit{how do we represent an SU in this reduced MAB?} Once an SU has been chosen by means of the reduced MAB, \textit{what offer should the PT make to the selected SU?}

There are two fundamental problems that prevent us from using previous MAB algorithms. The first problem is that the reward of our model does not match those of previous works. We encode the acceptance and rejection of offers as knowledge states. Thus, under a given policy, we can see the changes in the PT's knowledge as a Markov chain. Then, it is clear by (\ref{eq:W}) that the rewards of our reduced MAB come simultaneously from the probability distribution of $\gamma_s^{(n)}$ and from the Markov process of the knowledge states about the offers. Previous algorithms for MABs model the rewards when pulling an arm as drawn from a probability distribution (stochastic MABs), from a Markov chain (Markovian MABs), or chosen by an adversary (adversarial MABs). \textit{There are no algorithms in the literature for mixed stochastic and Markovian bandits}. Our proposals combine techniques from both types of bandits to solve the reduced MAB. The \textit{MAB-MDP algorithm} relies on a classic algorithm for stochastic MABs, augmented with an MDP, while the \textit{Super-UCB algorithm} is based on an algorithm for Markovian MABs augmented with elements from stochastic MABs. The main difference between our two algorithms is that Super-UCB integrates more information about SUs in the PT's policy than MAB-MDP as we will explain further in Section \ref{sec:MAB}.

The second problem when using classic MAB algorithms in our reduced MAB is that those algorithms only dictate one decision in every round (which arm to pull), whereas the PT needs to make one more decision (the offer to the SU).  The MDP of our MAB-MDP algorithm acts as an additional mechanism to the MAB to obtain such policy. In the case of Super-UCB, we make use of an extended model of Markovian MABs which handles MABs whose arms are MDPs instead of plain Markov chains, thus providing a policy for arm and action selection. 

% \subsection {Multi-armed bandit formulation}
% Mathematically, we model the sequential decisions and learning of the PT, choosing an SU to act as relay and what offer to make, as a multi-armed bandit (MAB) problem. 
% Let us denote the set of SUs present in the system as $\mathcal{S}$, and the set of possible offers as $\mathcal{A}$. 
% Our MAB problem consists of $|I|$ arms, where the set of arms $\mathcal{I}$ consists of the cartesian product of SUs and possible offers: $\mathcal{I} = \mathcal{S} \times \mathcal{A}$.

% In each decision phase $n$, the PU chooses to pull one of the arms (pick an SU and an offer) and it receives a reward $W$, that equals the utility of the PU:

% \[ W(s,\alpha) = \left\{ 
%   \begin{array}{l l}
%     (1-\beta)(1-\alpha)\log(1+\gamma_{ST,PR}) & \quad \text{if } \alpha \leq \alpha_{s}^{th}\\
%     (1-\beta)\log(1+\gamma_{PT,PR})& \quad \text{otherwise}
%   \end{array} \right.\]
  
% where $s \in \mathcal{S}$, $\alpha \in \mathcal{A}$.
% In our simulations we will consider that the reward when the offer is rejected is zero and recall that $\beta = 0$.

% The objective of the decision - maker (the PT) is to select one arm per round so that the total expected reward is maximized. This obviously means balancing immediate gains (pulling the arms that seem to be better in expectation) with gaining information to do better decisions in the next rounds (pulling arms that seem to be worse initially but could potentially be the best).
% A typical metric in learning problems is the regret, which is the total performance loss of a policy $\pi^n$ against an optimal policy $\pi^*$ pulling the best arm in average in every slot. The basic formulation of regret is: 
% \begin{equation}
% R^n = \mathbb{E}[\sum_n W^n_{\pi^*} - W^n_{\pi^{n}}]
% \end{equation}
% where
% \begin{equation}
% \pi^* = (s^*,\alpha^*) = \text{arg max}_{s,\alpha} \mathbb{E}[W(s,\alpha)]
% \end{equation}

% It is worth saying that the expectation of $W^n_{\pi^n}$ includes three random fenomena taking place: 1) the random values of the rewards, 2) the different decisions of a policy given the different observations and 3) randomized policies (as the $\epsilon$-greedy policy we use as a comparison in Section \ref{sec:Num}).
% For a detailed explanation of regret concepts, see \cite{ref:Bubeck2012}. 

% If we consider the combination $(s,\alpha)$ an arm of the multi-armed bandit problem, it is clear to see that the learning about the arms should not be treated independently.
% The reward obtained from an SU $s$ when offered a particular $\alpha$ will give the PT information about the rewards it can obtain with all the other offers / associated arms. 
% Let us go back to the example of the possibe offers $\mathcal{A} = \{0.3,0.5,0.7,0.9\}$. 
% Let us consider that the PT initial beliefs about the probability that an SU is of a particular type are uniformly distributed ($P(\theta_s = l) = 0.25, l \in \{1..4\}$). 
% Then, the probability of acceptance of each offer in $\mathcal{A}$ by an unknown SU at the decision round $n$ is $p_n = [0.25,0.50,0.75,1]$
% If the PT tries $\alpha = 0.5$ with an unknown SU and that SU rejects the offer, then the PT will learn that it is not a type-1 or type-2. Therefore, $p_{n+1} = [0,0,0.5,1]$.
% In addition,  the statistics of the rewards of the arms associated to the same SU (the arms representing an SU and each of the possible offers) are the same, multiplied by the constant $(1-\alpha_{1..4})$. 

% Inspired by \cite{ref:Pandey2007,ref:Brown2013}, we handle correlation by grouping the arms of an SU into \textit{clusters}, exploiting the mutual information. 

% In the following sections we describe two different scenarios addressed in our work: a toy scenario with a deterministic bandit setting, in which we consider the PT to have a full characterization of the distribution of the rewards and only has to discover the best offers to make , and the main scenario, a stochastic bandit configuration, in which we consider the PT also has to learn the mean parameter of the reward distributions as it goes. 

% \section{Deterministic bandit setting}\label{sec:Det}
% \textbf{Formulation as a dynamic program}. Let us consider that the PT knows the average SNR it can obtain from each SU in its coverage area but does not know the type of the SUs. 
% However, the PT has an initial estimation of the probability that each SU type is of a particular type. As an example, we are going to consider an uninformative prior belief, that is to say, we assume that the types are equiprobable. 

% We can formulate the search problem of discovering the optimal (SU,$\alpha$), with an exploration - exploitation trade-off, with dynamic programming (DP).  
% The state of the system, $\boldsymbol{x}$, is the information the PT has over the SUs in the system. We denote the information about an SU with $x$. 
% Then $\boldsymbol{x} = (x_1,...x_{|\mathcal{S}|})$, with $\mathcal|S|$ the number of SU pairs in the system. 
% For the case of uniform probabiity of the type of an SU, we encode such state with two variables, $(h_{rej},l_{acc})$. $h_{rej}$ is the index of the highest offer the SU has rejected and $l_acc$ is the index of the lowest offer that it accepted \footnote{High from the perspective of the SU, the one that would yield more profit to it}. 
% Please note that $h_{rej} < l_{acc}$. The PT knows that the SU will accept every offer with index $i \leq l_{acc}$ and reject those with index $i < h_{rej}$.
% %TO-DO: rephrase? Give an example?

% The transition probabilities for a given state are the PT's beliefs about the acceptance probabilities of each offer and SU. We consider the knowledge about SUs is not correlated in any way\footnote{But they could be correlated. For example, the PT may know there are SUs in its coverage area with similar behavior (because of some ``identifier'', such as the device type). In that case, the PT might exploit that correlation, updating its knowledge about all SUs with the same profile after interacting with only one of them.}, therefore, the change in state  $\boldsymbol{x}$ is a change in the state of a chosen SU $x$. In our example of assuming the type of an SU uniformly distributed, the acceptance probabilities of a particular SU $P(\alpha_s^* \leq \alpha_\pi)$ would be:

% \[ P(\alpha_s^* \leq \alpha_\pi) = \left\{ 
%   \begin{array}{l l}
%     0 & \text{for } s_\pi \leq h_{rej}\\
%     1 & \text{for } s_\pi > l_{acc}\\
%     \frac{s_\pi-h_{rej}}{l_{acc}-h_{rej}} & \text{for } h_{rej}>s_{\pi} \geq l_{acc} 
%   \end{array} \right.\]

% We can then formulate the Bellman equation as follows:

% \begin{equation}\label{eq:fullDP}
% V(\boldsymbol{x}) = \max_{i \in \mathcal{S}} \max_{\alpha \in \mathcal{A}}
%        \mathbb{E}[W(s,\alpha)+\delta \sum_{\boldsymbol{\tilde{x}} \in \mathcal{X}}
%        V(\boldsymbol{\tilde{x}}(\boldsymbol{x},\alpha))P_{\boldsymbol{x} \boldsymbol{\tilde{x}}}]
% \end{equation}

% where $\mathcal{X}$ denotes the set of all possible states and $\boldsymbol{\tilde{x}}$ represents the next stage state.

% The discount factor controls the balance of exploration - exploitation. Depending on the rate of change of the scenario around a particular PT, it may or may not be worth to spend much time learning about an SU. For example, the lifetimes of SUs in the PT's coverage area could be short (exploration is mostly worthless) or longer (exploitation can be delayed). 

% Although we can characterize the problem with DP, even for a small number of SUs and quantization of offers, it will suffer the curse of dimensionality and the model becomes too large for a exact solution. Let us consider only 4 possible offers: the state space of a single SU is 10. If there are 6 SUs in the coverage area, the state space grows to $10^6$.

% Focusing on the case of known average rewards ($W(s,\alpha) = \mu_{s,\alpha})$, the dynamics of the system are ``known'' (the PT does not have to learn the transition function or the rewards), it is less a learning problem and more a computational efficiency problem. 
% In the context of MABs, these problems are known as \textit{bandit optimization problems} \cite{ref:Tekin2013} or \textit{deterministic bandits}.
\section{MAB - MDP Algorithm}\label{sec:MAB}
In this proposal, we handle the reduced MAB as a stochastic MAB to obtain a policy for selecting which SU to bargain with, and integrate it with an MDP for the offer selection.

\textbf{Offer selection policy}.
We are interested in finding a policy $\pi_s^{\text{MDP}}$ that, given an SU $s\in\mathcal{S}$, maps the PT's knowledge about the type of $s$, to the next offer $\alpha$ to $s$.
%We can compute a policy for the PT to select the offers it is going to make to an SU by considering an SU $s$ in isolation.
Given $\alpha$, the reward $\psi$ that the MDP observes is $(1-\alpha)$ if the offer is accepted, and $0$ otherwise.
Therefore, a given policy $\pi_s^{\text{MDP}}$ induces a sequence of rewards $\psi^{(1)}, \psi^{(2)},\ldots,\psi^{(n)}$.
The uncertainty about the time horizon $n$ is captured by a discount factor $\delta<1$, characterizing the expected lifetime of the system, e.g., the probability that the $s$-PR pair remains active in each frame.
The search problem of discovering the optimal offer to $s$ consists of finding the policy maximizing $\mathbb{E}\left[\sum_{n=1}^{\infty}\delta^{n}\psi^{(n)}\right]$.
This problem is formulated as an MDP as follows.
%Let us assume a constant $\gamma_s = c$ for that SU (\textit{e.g.} $c = 1$). 

%We formulate the search problem of discovering the optimal $(\alpha_s^*)$ of SU $s$, with an exploration-exploitation tradeoff, by means of the following discounted Markov Decision Process (MDP) and associated Bellman equation:

%\begin{gather}
%\pi_s^{\mathcal{A}} = \argmax_{\pi} \sum_n \delta^{(n)} R(x,x',\pi(x)) \\
%¡V(x) = \max_{\alpha_a \in \mathcal{A}} \sum_{x' \in \mathcal{X}} P(x,x',\alpha_a)(R(x,x',\alpha_a)+\delta V(x'))
%\end{gather}

The set of \textit{states} of the MDP for SU $s$, denoted by $\mathcal{X}_s$, are all the possible knowledge states about the type of $s$. Assuming uniform probability for the type of an SU, the state of the MDP is completely defined by a two dimensional vector $x$, whose elements, $h$ and $l$, contain the index of the highest offer rejected by $s$, and the index of the lowest offer that $s$ accepted, respectively.
The PT knows that the SU accepts every offer $\alpha_a$ with index $a \geq l$, and rejects those with index $a \leq h$, $\alpha_a \in \mathcal{A}$.
The initial state is $(0,A)$, since the PT does not know anything about rejections initially ($h=0$) and $\alpha_A$ is known to be surely accepted ($l=A$).

Each \textit{transition probability} $P_s(x,x',\alpha_a)$ from a state $x$ to a state $x'$, with $x, x' \in \mathcal{X}_s$, given an offer $\alpha_a$, is determined by the PT's beliefs about the acceptance probability of the offer $\alpha_a$ at current knowledge state, $x$. This probability is defined as:
\begin{equation}
 P(\alpha_s^* \leq \alpha_a|x) = \left\{ 
  \begin{array}{l l}
    0 & \text{for } 0 < a \leq h\\
    \frac{a-h}{l-h} & \text{for } h < a < l \\
    1 & \text{for } l \leq a \leq A 
  \end{array} \right.
\end{equation}
Therefore, the transition probabilities between every pair of states are given by:
\begin{equation}
 P_s(x,x',\alpha_a) = \left\{ 
  \begin{array}{l l}
    P(\alpha_s^* \leq\alpha_a|x) & \text{for } a = l'\\
    1 - P(\alpha_s^* \leq\alpha_a|x) & \text{for } a = h'\\
    0 & \text{otherwise}
  \end{array} \right.
\end{equation}
Finally, the reward associated to a transition of the MDP is: 
\begin{equation}
\psi(x',\alpha_a) = \left\{ 
  \begin{array}{l l}
    1 - \alpha_a & \text{for } a = l'\\
    0 & \text{for } a = h'\\
  \end{array} \right.
\end{equation}
We can now formulate the Bellman equation that allows us to obtain the value function $V_s$ for each state:
\begin{equation}\label{eq:MDP}
%\begin{gather}
%\pi_s^{\mathcal{A}} = \argmax_{\pi} \sum_n \delta^{(n)} R(x,x',\pi(x)) \\
V_s(x) = \max_{\alpha_a \in \mathcal{A}} \sum_{x' \in \mathcal{X}_s} P_s(x,x',\alpha_a)(\psi(x',\alpha_a)+\delta V_s(x'))
%\end{gather}
\end{equation}
which can be readily solved offline by standard algorithms such as policy iteration \cite{ref:Bertsekas2005}.
\begin{figure}[!t]
\centering
\includegraphics[scale=0.9]{Esquema.eps}
\caption{MAB-MDP algorithm for $S$ SUs and $A=3$ possible offers . Each SU is chosen according to a MAB and the offer according to an MDP. ``acc'' represents the event that the offer is accepted and ``rej'' that it is rejected.}
\label{fig:Esquema}
\vspace{-1.5em}
\end{figure}

\textbf{SU representation in the reduced MAB}. In a classic stochastic MAB, rewards when pulling an arm are drawn from a probability distribution associated to that arm. Proposed policies in the literature compute an index for each arm (dependent on that arm only), $I_s^{(n)}$, and dictate to pull the arm with the highest index on each decision round. A commonly used family of index policies are the \textit{Upper Confidence Bound (UCB)} policies proposed by \cite{ref:Auer2002}. 
%Let $W^{n}_s$ denote the subsequence of $W^{n}$ corresponding to the rewards obtained from SU $s$.
The index of these policies consists of the sample average reward obtained from arm $s$ up to round $n$, plus an additional term, the UCB, related to the uncertainty of that estimation. 

In our case, the reward $W$ when pulling arm $s$ depends not only on $s$ but also on $\alpha$: $W^{(n)}(s,\alpha) = (1-\alpha)\text{log}(\Gamma_s^{(n)})$. When choosing arm $s$ and offering $\alpha$, the reward is drawn from the Gaussian distribution $\text{log}(\Gamma_s^{(n)}) \sim N(\mu_{\gamma_s},\sigma_{\gamma_s})$, multiplied by a constant $(1-\alpha)$. 
Let us denote by $x_s^{(n)}=(h_s^{(n)},l_s^{(n)}) \in \mathcal{X}_s$ the state of SU $s$ at round $n$.
We characterize the SU in the reduced MAB by its \textit{presumed best achievable offer} $\alpha^{\text{min}}(x_s^{(n)})$, denoting the minimum $\alpha \in \mathcal{A}$ in state $x_s^{(n)}$ with positive belief of being accepted and also being achievable by the offer selection policy $\pi_s^{\text{MDP}}$.\footnote{For a policy $\pi_s^{\text{MDP}}$ that ends up exploring all the offers of an SU, $\alpha^{\text{min}}(x_s^{(n)})$ is simply the lowest $\alpha$ not rejected for the current state $x$. For more conservative policies that may dictate not to explore all offers, $\alpha^{\text{min}}(x_s^{(n)})$ is the lowest $\alpha$ that policy is willing to explore given the state $x_s^{(n)}$. The complete notation would be $\alpha^{\text{min}}(x_s^{(n)},\pi_s^{\text{MDP}})$ but we use the former to improve readability.} 

Note that, in general, $\alpha^{\text{min}}(x_s^{(n)}) \neq \alpha_a$, with $\alpha_a = \pi_s^{\text{MDP}}(x_s^{(n)})$. That is, the PT uses an optimistic representation of SU $s$ type in a given state $x_s{(n)}$, but the PT may not directly jump into the corresponding optimal offer $\alpha^{\text{min}}$. Instead, the PT may approach it in successive interaction rounds by reducing the offer every time, gradually increasing the risk of rejection, in order to balance exploration and exploitation as dictated by $\pi_s^{\text{MDP}}$.

By applying this SU characterization to the UCB index for Gaussian distributions shown in \cite{ref:Auer2002}, we obtain:
\begin{equation}\label{eq:index}
I_{s}^{(n)} = \widehat{W}^{(n)}_s + 4 \hat\sigma_{W_{s}}(x_s^{(n)})\sqrt{\frac{\ln(n_{\left\{W>0\right\}} + 2)}{n_s+1}}
\end{equation}
where $\widehat{W}^{(n)}_s$ is the estimated average reward of arm $s$ up to round $n$, and $\hat\sigma_{W_{s}}(x_s^{(n)})$ is the estimated standard deviation of the rewards of arm $s$, $n_s$ is the number of times arm $s$ has been pulled up to round $n$, and $n_{\left\{W>0\right\}} $ is the number of rounds in which the PT obtained a positive reward from any SU. Since $s$ is characterized by its presumed best achievable offer in state $x_s^{(n)}$, we have that $\widehat{W}_s^{(n)} = (1-\alpha^{\text{min}}(x_s^{(n)}))\overline\gamma_s^{(n)}$ and $\hat\sigma_{W_s}(x_s^{(n)}) = (1-\alpha^{\text{min}}(x_s^{(n)}))\sigma_{\gamma_s}$, with $\overline\gamma_s^{(n)}$ and $\sigma_{\gamma_s}$ denoting the sample mean and known standard deviation of $\gamma_s^{(n)}$, respectively.

In practice, removing the factor $4$ from the UCB yields a significantly lower regret, while still achieving convergence. However, despite this empirical evidence, there are no theoretical bounds on the regret for this version of the UCB.

We proceed to give a detailed \textbf{description of the MAB - MDP algorithm}. Its diagram and pseudo-code can be found in Fig. \ref{fig:Esquema} and Algorithm \ref{code:TLP}, respectively. Initially, for each SU $s \in \mathcal{S}$:
\begin{enumerate}
\item The PT computes the exploration-exploitation policy $\pi_s^{\text{MDP}}$ for the offers $\alpha \in \mathcal{A}$. 
\item Given that policy, for the initial state $x_s^{(0)} = (0,A)$, the PT computes the presumed best achievable offer $\alpha^{\text{min}}(x_s^{(0)})$ of each SU. The PT chooses that offer as representative of the SU. 
\item With $\alpha^{\text{min}}(x_s^{(0)})$ and the initial sample of the SNR $\gamma^{(0)}_s$,
%\footnote{We could also leverage this constraint by simply assuming an initial belief (with much uncertainty) about an SU. However, this initial sample is a common practice in most multi-armed bandit algorithms, whose initialization is by pulling all arms first.}
the PT builds the UCB index $I_{s}^{(0)}$ (\ref{eq:index}).
\newcounter{enumTemp}
\setcounter{enumTemp}{\theenumi}
\end{enumerate}
Then, on each round $n$:
\begin{enumerate}
\setcounter{enumi}{\theenumTemp}
\item The PT selects the SU $z$ with the highest UCB index $I_{z}^{(n)}$ and the offer $\alpha_a \in \mathcal{A}$ is indicated by the MDP policy $\pi_z^{\text{MDP}}$.
\item If the SU \textit{rejects} the offer, the PT updates its knowledge state (MDP) $x_z^{(n+1)}$ and checks if $\alpha^{\text{min}}(x_z^{(n+1)}) \neq \alpha^{\text{min}}(x_z^{(n)})$. If that is the case, the PT computes $I_{z}^{(n+1)}$ according to (\ref{eq:index}). Otherwise, $I_{z}^{(n+1)} = I_{z}^{(n)}$.
%(the average reward $\overline{W}_s^{(n)}$ and standard deviation $\overline\gamma_s^{(n)}$ get multiplied by the new $(1-\alpha^{\text{min}}(x_{i^{(n)}}))$ factor).
\item If the SU \textit{accepts}, the PT updates $x_z^{(n+1)}$, and sets $n_z =n_z+1$. The indices $I_{s}$, for all $s\in\mathcal{S}$, are also updated because $n_{\left\{W>0\right\}}  = n_{\left\{W>0\right\}}+1$ after the PT receives a positive reward.\footnote{In a classic stochastic MAB, every arm pull increases $n_{\left\{W>0\right\}}$  because every arm pull gives a reward from a probability distribution. This is not our case due to the possibility that an offer is rejected.}
\end{enumerate}

Note that it cannot be assured that this algorithm performs a fully optimal learning. An optimal learning algorithm would imply solving an MDP comprising all the information gathered by the PT. Our MAB-MDP algorithm decomposes the learning problem into two simpler sub-problems (the MAB and the MDP). Each sub-problem can be solved very efficiently because it uses only partial information about the global system. Despite its simplicity, MAB-MDP shows a remarkably low regret in numerical evaluation.
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{figure}
\begin{algorithmic}[1]
  \REQUIRE $\mathcal{X}_s,\mathcal{A},P_s,\gamma_s^{(0)};\text{ } \forall s \in \mathcal{S}$
  \ENSURE $(s^{(n)},\alpha^{(n)});\text{ } s^{(n)} \in \mathcal{S}, \alpha^{(n)} \in \mathcal{A};\text{ } \forall n$
  \STATE Initialize. $\forall s \in \mathcal{S}$: 
    \STATE\hspace{\algorithmicindent}$n_{\left\{W>0\right\}}, n_s^{(n)}, n \leftarrow 0$
    \STATE\hspace{\algorithmicindent}$\pi_s^{\text{MDP}} \leftarrow$ Solve MDP $(\mathcal{X}_s,\mathcal{A},P_s)$
    \STATE\hspace{\algorithmicindent}$x_s^{(0)} \leftarrow (0,A)$
    \STATE\hspace{\algorithmicindent}$\widehat{W}_s^{(0)}\leftarrow (1-\alpha^{\text{min}}(x_s^{(0)}))\gamma_s^{(0)}$ 
    \STATE\hspace{\algorithmicindent}$\hat\sigma_{W_s}(x_s^{(0)}) \leftarrow (1-\alpha^{\text{min}}(x_s^{(0)}))\sigma_{\gamma_s}$
    \STATE\hspace{\algorithmicindent}$I_{s}^{(0)} = \widehat{W}^{(0)}_s + 4 \hat\sigma_{W_{s}}(x_s^{(0)})\sqrt{\frac{\ln(n_{\left\{W>0\right\}} + 2)}{n_s+1}}$
  \LOOP
    \STATE $s^{(n)} \leftarrow z \leftarrow \underset{s}\max\text{ }I_s^{(n)}$
    \STATE $\alpha^{(n)} \leftarrow \alpha_a \leftarrow \pi_z^{\text{MDP}}(x_z^{(n)})$\renewcommand{\algorithmicensure}{\textbf{Output:}}
    \IF[SU rejects] {$\alpha_a < \alpha_{z}^*$} 
      \STATE collect reward  $0$
      \STATE $x_z^{(n+1)} \leftarrow (a,l_z^{(n)})$ 
      \STATE $\widehat{W}_z^{(n+1)}\leftarrow (1-\alpha^{\text{min}}(x_z^{(n+1)}))\overline{\gamma}_z^{(n)}$
      \STATE $\hat\sigma_{W_z}(x_z^{(n+1)}) \leftarrow (1-\alpha^{\text{min}}(x_z^{(n+1)}))\sigma_{\gamma_z}$
      \STATE $I_{z}^{(n+1)} \leftarrow \widehat{W}_z^{(n+1)} + 4\hat\sigma_{W_z}(x_z^{(n+1)}) \sqrt{\frac{\ln(n_{\left\{W>0\right\}} + 2)}{n_z+1}}$ 
    \ELSE
      \STATE collect reward $(1-\alpha_a)\log(\Gamma_z^{(n)})$
      \STATE $x_z^{(n+1)} \leftarrow (h_z^{(n)},a)$
      \STATE $\overline{\gamma}_{z}^{(n+1)} \leftarrow \frac{\overline{\gamma}_{z}^{(n)}n_z+\gamma_z^{(n)}}{n_z+1}$  
      \STATE $\widehat{W}_z^{(n+1)}\leftarrow (1-\alpha^{\text{min}}(x_z^{(n)}))\overline{\gamma}_z^{(n+1)}$
      \STATE $n_z \leftarrow n_z + 1$
      \STATE $n_{\left\{W>0\right\}} \leftarrow n_{\left\{W>0\right\}} + 1$
      \FORALL{$s \in \mathcal{S}$}
        \STATE $I_{s}^{(n+1)} \leftarrow \widehat{W}_{s}^{(n+1)} + 4\hat\sigma_{W_s}(x_s^{(n)}) \sqrt{\frac{\ln(n_{\left\{W>0\right\}} + 2)}{n_s+1}}$
      \ENDFOR
    \ENDIF
    \STATE $n \leftarrow n + 1$
  \ENDLOOP
\end{algorithmic}
\caption{MAB-MDP algorithm in pseudo-code}
\label{code:TLP}
\end{figure}
%This formulation does not perform an optimal learning of both the average SNR $\mu_{\gamma_s}$ and the type of an SU. The MDP of each arm does not consider the possibility of the PT of choosing a different SU, or how interacting several times with the same has an impact on the estimate $\overline{\gamma_s}^{(n)}$, although this is captured in some way by the stochastic MAB. And viceversa in the MAB decision: when choosing the SU, the UCB index is not considering all the information the PT knows about the SUs acceptance of offers, just the maximum offer $(1-\alpha_{x}^{min})$ the PT believes it can get from each SU. But the exploration-exploration trade-off in discovering the optimal offer is considered by the MDP.

\section{Super-UCB Algorithm}\label{sec:Super}
The MAB-MDP algorithm of the previous section is based on a stochastic MAB. In this section we follow the Markovian MAB approach. Our objective is to profit from all the information gathered about the SU types, instead of considering only their presumed best achievable offer.

For the classic Markovian MAB with independent arms in which a decision-maker (the PT in our scenario) selects an arm to play and receives a random reward from a Markov chain, Gittins \cite{ref:Gittins2011} developed an index policy proven to be optimal if the transition probabilities are known.
The computation of the indices for each state of each arm is performed offline. 
The online sequential decision is as simple as in the stochastic MAB index policies: at each decision stage, the decision-maker has to pull the arm with the highest index for their current states. 
Then, the state of the pulled arm changes and so does the index of that arm, which is replaced by the one associated to the new state. 

\textbf{Superprocess}. 
As we are grouping the arms of the MAB by SU, the rewards when choosing an SU are not drawn from Markov reward chains but from Markov Decision Processes (MDP). The PT does not only select which SU to interact with (pulling an arm) but also which offer to make (selecting an action of the MDP). 
If we consider the SUs as independent, this problem can be formulated as an extension to MABs called \textit{superprocess}. P. Whittle \cite{ref:Whittle1980} extended the Gittins index policies to superprocesses. Regarding the offer selection policy, there are different choices that we discuss later.


%Note that the MDPs of the arms in this case will differ from the MDP described in eq. (\ref{eq:MDP}) in that now we are not focusing only in the policy for the offers. This MDP now
Ideally, such MDP of an arm should encode all information or beliefs the PT has about an SU, that is, both the acceptance and rejection of offers, and the samples of $\gamma_s^{(n)}$. 
Unfortunately, including the information about $\gamma_s^{(n)}$ would lead to a continuous-state MDP which is intractable in practice. 
Combining the superprocess with Approximate Dynamic Programming techniques is not an option either: as shown in \cite{ref:Pandey2007}, the approximations obtained for Gittins indices are not tight enough (not even to be used as heuristics). In order to integrate the uncertainty about $\mu_{\gamma_s}$ in the superprocess, without making the state space explode, we propose an approximate method by including the PT's beliefs about $\mu_{\gamma_s}$ in the reward of the MDP in the form of a UCB index, instead of including them as state variables. Then, in every round in which the UCB indices get updated, that is, rounds in which the PT's beliefs about $\mu_{\gamma_s}$ change, the PT solves the superprocess using the new UCB indices in the rewards of the MDPs, obtaining a new set of Gittins indices for all states.


\textbf{Solution to the superprocess.} In order to obtain the Gittins index for state $x$ of an arm $s$, Whittle introduces a retirement action into the Bellman equation.
The retirement action leads to a ficticious final state with a unique reward $M$. Then, the value of state $x$ at round $n$, $V_s^{(n)}(x,M)$, is obtained by solving:
\begin{equation}\label{eq:MDPret}
V_s^{(n)}(x,M) = \max\left\{M, \max_{\alpha_a \in \mathcal{A}} \sum_{x' \in \mathcal{X}_s} P_s(x,x',\alpha_a)\left(\phi_s^{(n)}(x',\alpha_a)+\delta V_s^{(n)}(x',M)\right)\right\}
\end{equation}
where $\phi_s^{(n)}$ denotes the reward function at round $n$, associated to a transition of the MDP of SU $s$, and it is defined as follows:
\begin{equation}
\phi_s^{(n)}(x',\alpha_a) = 
\begin{cases}
    (1 - \alpha_a) J_s^{(n)} & \text{for } a = l'\\
    0 & \text{for } a = h'
   \end{cases}
\end{equation}

and $J_s^{(n)}$ denotes a UCB index of the form $J_s^{(n)} = \left(\overline{\gamma}_{s}^{(n)}+4\sigma_{\gamma_s}\sqrt{\frac{\ln{(n_{\left\{W>0\right\}}+2)}}{n_s+1}}\right)$.
The \textit{Gittins index} $M_s^{(n)}(x)$ for SU $s$ in state $x$ and round $n$ is the smallest $M$ that makes the PT indifferent between taking the retirement action or continuing in the sequential decision problem. The retirement reward symbolizes the reward that could be achieved by switching to the other arms.

In order to find the Gittins indices $M_s^{(n)}(x), \forall x \in \mathcal{X}_s$, (\ref{eq:MDPret}) has to be repeatedly solved for different values of $M$. Following the approach in \cite{ref:Brown2013}, we can formulate (\ref{eq:MDPret}) as a linear program \cite{ref:Bertsekas2005} and use the techniques of parametric linear programming with parameter $M$.
Let $\boldsymbol{v}_s^{(n)}$ denote the vector of values representing the value function $V_s^{(n)}(x,M),\text{ }\forall x \in \mathcal{X}_s$. The corresponding linear program to (\ref{eq:MDPret}) for SU $s$ at round $n$ is: 
\begin{equation}
\begin{split}
  &\underset{\boldsymbol{v}_s^{(n)}}{\text{minimize }}\sum_{x \in \mathcal{X}_s} v_s^{(n)}(x)\\
  &\text{subject to }v_s^{(n)}(x) \geq \sum_{x' \in \mathcal{X}_s}  P_s(x,x',\alpha_a)(\phi_s^{(n)}(x,x',\alpha_a)\\
  &\hphantom{\text{subject to }v_s^{(n)}(x) \geq \sum_{x' \in \mathcal{X}_s} P_s}+\delta v_s^{(n)}(x')), \forall x \in \mathcal{X}_s,\alpha_a \in \mathcal{A}\\
  &\hphantom{\text{subject to }}v_s^{(n)}(x)\geq M \text{,    }\forall x \in \mathcal{X}_s
\end{split}
\end{equation}

$V_s^{(n)}(x,M)$ is known to be piecewise linear, increasing, and convex in $M$ \cite{ref:Brown2013}. Obtaining the value of $V_s^{(n)}(x,M)$, then, consists of finding the slopes and breaking points of the pieces. Using the Gaas-Saaty algorithm for parametric linear programming \cite{ref:Gass1955}, the authors of \cite{ref:Brown2013} proposed the \textit{Frontier Algorithm} to compute the Gittins indices $M_s^{(n)}(x)$. The Gaas-Saaty algorithm is a variation of the traditional simplex method for linear programs.

Given (\ref{eq:MDPret}) and the particular form of $\phi_s^{(n)}$, there is no need to solve the superprocess on each round in response to the changes in $J_s^{(n)}$. The next result shows that it is enough to solve Eq. (\ref{eq:MDPret}) once per SU, using a reference index $J_s^{\prime(n)} = 1$.
\begin{prop}
Let $J_s^{(n)}$ be the UCB index of an SU $s$ at round $n$, and let $\mathcal{X}_s$ be the state space of $s$. The Gittins indices for each state $x \in \mathcal{X}_s$ at round $n$ are given by:
\begin{equation}
M_s^{(n)}(x) = J_s^{(n)}M'_s(x)
\end{equation}  
where $M'_s(x)$ is the Gittins index for state $x$ when the UCB index is $J_s^{\prime(n)} = 1$.
\end{prop}
\begin{proof}
Consider Eq. (\ref{eq:MDPret}) for round $n$ of SU $s$. Dividing both sides of the expression by $J_s^{(n)}$, we obtain:
\begin{equation}\label{eq:MDPretproof}
\frac{V_s^{(n)}(x,M)}{J_s^{(n)}} = \max\left\{\frac{M}{J_s^{(n)}},\max_{\alpha_a \in \mathcal{A}} \sum_{x' \in \mathcal{X}_s} P_s(x,x',\alpha_a)\left(\frac{\phi_s^{(n)}(x',\alpha_a)}{J_s^{(n)}}+\delta \frac{V_s^{(n)}(x',M)}{J_s^{(n)}}\right)\right\}
\end{equation}
Observe that $\frac{\phi_s^{(n)}(x',\alpha_a)}{J_s^{(n)}} = \phi_s^{\prime(n)}(x',\alpha_a)$, where:
\begin{equation}
\phi_s^{\prime(n)}(x',\alpha_a) = 
\begin{cases}
    (1 - \alpha_a) J_s^{\prime(n)} & \text{for } a = l'\\
    0 & \text{for } a = h'
   \end{cases}
\end{equation}
Denoting $\frac{V_s^{(n)}(\cdot,M)}{J_s^{(n)}} = V_s^{(n)}(\cdot,\frac{M}{J_s^{(n)}})$, Eq. (\ref{eq:MDPretproof}) becomes:
\begin{equation}
\begin{array}{ll}
V_s^{(n)}\left(x,\frac{M}{J_s^{(n)}}\right) = \max & \left\{\frac{M}{J_s^{(n)}},\right.\\
&\left.\underset{\alpha_a \in \mathcal{A}}\max \sum_{x' \in \mathcal{X}_s} P_s(x,x',\alpha_a)\left(\phi_s^{\prime(n)}(x',\alpha_a)+\delta V_s^{(n)}\left(x',\frac{M}{J_s^{(n)}}\right)\right)\right\}
\end{array}
\end{equation} 
which is essentially equal to Eq. (\ref{eq:MDPret}), with $J_s^{\prime(n)} = 1$ and $M$ scaled by a factor $J_s^{(n)}$. It is then straightforward to see that $\frac{M_s^{(n)}(x)}{J_s^{(n)}} = M'_s(x)$.
\end{proof}
This fact greatly reduces the computation overhead of the algorithm, because the solution to the superprocess only needs to be computed once, and can be done offline.

\textbf{Offer selection policy.} Regarding the action to take on a given state for an arm, there are different choices.
The Whittle - Gittins index policy for superprocesses turns out to be optimal only if there is a dominant action $\alpha_s^{\text{DOM}}(x)$, for each arm $s$ and state $x \in \mathcal{X}_s$, that achieves the maximum in (\ref{eq:MDPret}) for all values of $M \in [0,M_s'(x)]$. 
If that is the case, the action indicated by the policy is the dominant action. In our case, however, the action indicated by the policy solving (\ref{eq:MDPret}) changes with the retirement value. 
This means that the choice of the action for a given SU depends on what the PT knows about the others (\textit{i.e.,} depending on the types of the other SUs, the PT may decide to explore more or less a particular SU). Thus, it is suboptimal to consider each SU in isolation when deciding on the offer to make.
Nevertheless, even for this suboptimal case, the Gittins - Whittle policy can still be used as a heuristic, providing tight upper bounds (< 2\%) on the values of $V_s^{(n)}(x)$ (Eq. (\ref{eq:MDPret})) as reported by \cite{ref:Brown2013}.
For the suboptimal cases, \cite{ref:Pandey2007} suggests to choose the actions that maximize $V_s^{(n)}(x, M_s'(x))$. However, such policy leads to under-exploration of an SU, while a policy selecting the actions that maximize $V_s^{(n)}(x,0)$ in (\ref{eq:MDPret}) attains a lower regret, as noticed in \cite{ref:Brown2013} and verified by our simulations.\footnote{This does not mean that the PT never switches arms, as that is dictated by the Gittins indices. Rather, the offer selection policy explores an SU as if the PT never switched SU.} We will denote such policy by $\pi_s^{\text{SPR}}$. The Frontier Algorithm provides all the previously mentioned policies. 

\textbf{Description of the Super-UCB algorithm.}
\begin{figure}[!t]
\centering
\includegraphics[scale=0.8]{esquemaSuper.eps}
\caption{Super-UCB algorithm for $S$ SUs and $A=3$ possible offers. SUs and offers are chosen according to a MAB superprocess, whose reward includes a UCB index as a factor. ``acc'' represents the event that the offer is accepted and ``rej'' that it is rejected.}
\label{fig:esquemaSuper}
\end{figure}
Its diagram and pseudo-code can be found in Fig. \ref{fig:esquemaSuper} and Algorithm \ref{code:FA_UCB}, respectively. Initially, for each SU $s \in \mathcal{S}$:
\begin{enumerate}
\item We obtain the reference Gittins indices $M_s'(x)$ and the offer selection policy $\pi_{s}^{SPR}$ for every state $x$ of SU $s$, solving (\ref{eq:MDPret}) with the Frontier Algorithm.
\item The PT computes the UCB index $J_s^{(0)}$ and the Gittins indices $M_s^{(0)}$ with the initial samples $\gamma_s^{(0)}$. \footnote{We use $M_s^{(0)}$ for $M_s^{(0)}(x), \text{ } \forall x \in \mathcal{X}_s$ to improve readability.}
\newcounter{enumTempB}
\setcounter{enumTempB}{\theenumi}
\end{enumerate}
Then, on each round $n$:
\begin{enumerate}
\setcounter{enumi}{\theenumTempB}
\item The PT selects the SU $z$ with the highest Gittins index $M_z^{(n)}(x_z^{(n)})$, $z \in \mathcal{S}$, and the offer $\alpha_a \in \mathcal{A}$ indicated by $\pi_{z}^\text{SPR}$.
\item If the SU \textit{rejects} the offer, the PT updates its current knowledge state $x_z^{(n+1)}$, and the SU is represented by the Gittins index $M_z^{(n+1)}(x_z^{(n)})$ of the new state.
%(the average reward $\overline{W}_s^{(n)}$ and standard deviation $\overline\gamma_s^{(n)}$ get multiplied by the new $(1-\alpha^{\text{min}}(x_{i^{(n)}}))$ factor).
\item If the SU \textit{accepts}, the PT updates $x_z^{(n+1)}$, the SU is represented by the Gittins index $M_z^{(n+1)}(x_z^{(n)})$ of the new state, and the PT sets $n_{z}=n_{z}+1$. The indices $J_{s}^{(n+1)}$ for all $s\in\mathcal{S}$, are also updated because $n_{\left\{W>0\right\}}  = n_{\left\{W>0\right\}}+1$ after the PT receives a positive reward.
\end{enumerate}

Note that it is still not assured that the algorithm performs fully optimal learning, which, as we have already seen, is intractable.
Nevertheless, \textit{Super-UCB makes a more effective use of the available information with respect to the MAB-MDP algorithm.} This is reflected in a regret gap as shown in Section \ref{sec:Num}. When choosing which SU $s$ to bargain with, Super-UCB takes into account all the PT knowledge about the type of an SU, while MAB-MDP represents each SU by its presumed best achievable offer (or best possible type given the observations). This makes MAB-MDP more prone to over-exploration of different SUs. To illustrate this, let us consider the following example comprising two SUs, $s_1$ and $s_2$. At a given round, PT knows that $s_1$ accepts $\alpha_2$ but has not tried $\alpha_1$ on $s_1$ yet ($\alpha_1 < \alpha_2$). No offer has been made to $s_2$ so far. For MAB-MDP, both SUs are assumed to be type-1, since both of them can potentially accept $\alpha_1$. Therefore, the choice made by MAB-MDP in this case would only be determined by PT's estimations of $\mu_{\gamma_{s_1}}$ and $\mu_{\gamma_{s_2}}$, that is, the sample means $\overline\gamma_{s_1}^{(n)}$ and $\overline\gamma_{s_2}^{(n)}$, and the number of samples $n_{s_1}$ and $n_{s_2}$ at round $n$, according to (\ref{eq:index}). Knowing that $s_1$ accepts at least $\alpha_2$ would have no influence on the choice made.

Note that, when computing $\widehat{W}_s^{(n)}$ in MAB-MDP, if we represent the SUs by their average type instead of the presumed best possible type, the algorithm becomes greedier, making decisions mostly based on average rewards. Then, MAB-MDP would incur in over-exploitation, and its performance in terms of regret would be notably worse. The inconvenience of basing decisions on average rewards is well reported in reinforcement learning literature \cite{ref:Sutton1998}. \textit{Super-UCB, however, can use the information about SU types in a more balanced way thanks to the retirement action in the superprocess formulation}. The Gittins indices $M_s^{(n)}$ of an SU $s$ are related to the expected reward of interacting with $s$, but that expectation takes into account the possibility of not exploring that SU infinitely and switching to a potentially more rewarding one. 

\begin{figure}
\begin{algorithmic}[1]
  \REQUIRE $\mathcal{X}_s,\mathcal{A},P_s,\gamma_s^{(0)};\text{ } \forall s \in \mathcal{S}$
  \ENSURE $(s^{(n)},\alpha^{(n)});\text{ } s^{(n)} \in \mathcal{S}, \alpha^{(n)} \in \mathcal{A};\text{ } \forall n$
  \STATE Initialize. $\forall s \in \mathcal{S}$: 
    \STATE\hspace{\algorithmicindent}$n_{\left\{W>0\right\}}, n_s^{(n)}, n \leftarrow 0$
    \STATE\hspace{\algorithmicindent}$[{M_s'},\pi_s^{\text{SPR}}] \leftarrow Frontier Algorithm(\mathcal{X}_s,\mathcal{A},P_s)$   
    \STATE\hspace{\algorithmicindent}$x_s^{(0)} \leftarrow (0,A)$
    \STATE\hspace{\algorithmicindent}$J_s^{(0)} = \gamma_{s}^{(0)}+4\sigma_{\gamma_s}\sqrt{\frac{\ln{(n_{\left\{W>0\right\}}+2)}}{n_s^{(n)}+1}}$    
    \STATE\hspace{\algorithmicindent}$M_s^{(0)} \leftarrow M_s'J_s^{(0)}$
  \LOOP
    \STATE $s^{(n)} \leftarrow z \leftarrow \underset{s}\max{\text{ }}M_s^{(n)}(x_s^{(n)})$
    \STATE $\alpha^{(n)} \leftarrow \alpha_a$ $\leftarrow \pi_s^{SPR}(x_z^{(n)})$
    \IF[SU rejects] {$\alpha_a < \alpha_{z}^*$} 
      \STATE collect reward  $0$
      \STATE $x_z^{(n+1)} \leftarrow (a,l_z^{(n)})$
    \ELSE
      \STATE collect reward $(1-\alpha_a)\log(1+\Gamma_{z}^{(n)})$
      \STATE $x_{z}^{(n+1)} \leftarrow (h_z^{(n)},a)$ 
      \STATE $\overline{\gamma}_{z}^{(n+1)} \leftarrow \frac{\overline{\gamma}_{z}^{(n)}n_z+\gamma_z^{(n)}}{n_z+1}$  
      \STATE $n_{z} \leftarrow n_{z} + 1$
      \STATE $n_{\left\{W>0\right\}} \leftarrow n_{\left\{W>0\right\}} + 1$
      \FORALL{$s \in \mathcal{S}$}  
        \STATE $J_s^{(n+1)} \leftarrow \overline{\gamma}_{s}^{(n+1)} + 4\sigma_{\gamma_s} \sqrt{\frac{\ln{(n_{\left\{W>0\right\}}+2)}}{n_s+1}}$
        \STATE $M_s^{(n+1)} \leftarrow M_s'J_s^{(n+1)}$
      \ENDFOR
    \ENDIF
    \STATE $n \leftarrow n+1$
  \ENDLOOP
\end{algorithmic}
\caption{Super-UCB algorithm in pseudo-code}
\label{code:FA_UCB}
\end{figure}

\section{Numerical Results}\label{sec:Num}
In addition to the MAB-MDP, we use two more algorithms as benchmarks for Super-UCB: an $\epsilon$-greedy algorithm and a standard superprocess which we call Super-0CB. We evaluate them over time, versus the standard deviation of the SNR of the channels $\sigma_{\gamma_s}$, the number of SUs $S$ and the number of offers $A$. We also observe the robustness of MAB-MDP and Super-UCB against mis-estimations of $\sigma_{\gamma_s}$.

\textbf{Comparison with $\epsilon$-greedy}.
Given the main features of our proposed mechanism (real-time operation and exploration-exploitation tradeoff), the best candidates for comparison are the families of reinforcement learning heuristics \cite{ref:Vermorel2005,ref:Sutton1998} known as $\epsilon$-greedy policies.
Specifically, we will compare our algorithm to the $\epsilon$-linear-descending strategy. 
This strategy chooses the alternative $u = (s,\alpha)$ with best expected reward with probability $1-\epsilon^{(n)}$, and performs random exploration with probability $\epsilon^{(n)}$, which, in each round $n$, takes the value $\epsilon^{(n)} = \min\left(\epsilon^{(0)}/n,1\right)$, or $\epsilon^{(n)} = \min\left(\epsilon^{(0)}\ln(n)/n,1\right)$ for a log-descending variant, where $\epsilon^{(0)}$ is a tunable parameter. The alternative $u$ is chosen according to: $u = (s,\alpha) = \argmax_{s,\alpha} (1-\alpha)\overline{\gamma}^{(n)}_s P(\alpha_s^* \leq \alpha)$.
As noted in the literature \cite{ref:Auer2002,ref:Vermorel2005}, the $\epsilon$-descending greedy strategy performs as well as (and most of the time, even better than) many other complex policies. 
Its main drawback is that the $\epsilon^{(0)}$ parameter has to be carefully chosen and there is little that can be said theoretically about its optimal value except for distributions with support on $[0,1]$ (see \cite{ref:Auer2002}). 
Based on empirical tests of a wide range of values for $\epsilon^{(0)}$, we set it to $10$ for all figures. 

\textbf{Comparison with a superprocess without upper confidence bounds (Super-0CB)}.
We also compare to an algorithm solving the superprocess in (\ref{eq:MDPret}) with $J_s^{(n)} = \overline{\gamma}_s^{(n)}$ at every round $n$, which we will call \textit{Super-0CB}. This is equivalent to compute the Gittins indices at every $n$ assuming that the true values of $\mu_{\gamma_s}^{(n+i)},\text{ } i = 0,1,2,...,\text{ }\forall s \in \mathcal{S}$, are the sample means observed up to round $n$, that is, assuming zero upper confidence bound. 
We set the discount factor $\delta$ of MAB-MDP, Super-UCB, and Super-0CB to 0.98.
The information taken into account by each algorithm is summarized in Table \ref{Table:info}.
\begin{table}
\begin{threeparttable}
\small
\caption{Summary of the information exploited by each algorithm}
\label{Table:info}
\begin{tabulary}{10cm}{lccC{4.7cm}C{4.7cm}}
\hline
& $\overline\gamma_s$ & $\sigma_{\gamma_s}$ & Possible SU types for SU selection & Possible SU types for offer selection \\ \hline
$\epsilon$-greedy & \checkmark & $\times$ & / & / \\ 
Super-0CB         & \checkmark & $\times$ & \checkmark & \checkmark \\
MAB-MDP           & \checkmark & \checkmark & / & \checkmark \\ 
Super-UCB         & \checkmark & \checkmark & \checkmark & \checkmark \\ \hline
\end{tabulary}
\begin{tablenotes}
\item \hspace{6em} \checkmark = exploited, / = partly exploited, $\times$ = not exploited
\end{tablenotes}
\end{threeparttable}
\end{table}

\subsubsection{Performance over time}
\begin{figure}[!t]
\centering
\includegraphics{stoVsT.eps}
\caption{Regret over time for the MAB-MDP, Super-UCB, Super-0CB and the $\epsilon$-greedy algorithms, for $S=7$ SUs and $A=7$ possible offers. The standard deviation of the SNR of each arm $s$ is $\sigma_{\gamma_s} = 5$ dB and the average SNRs $\mu_{\gamma_s}$ are chosen uniformly from the range $[5,40]$ dB. The results are averaged over 2000 independent experiments.}
\label{fig:stoVsT}
\end{figure}

Fig. \ref{fig:stoVsT} shows the performance of our two proposals MAB-MDP and Super-UCB over time compared to $\epsilon$-greedy. 

As expected, MAB-MDP and Super-UCB outperform $\epsilon$-greedy because they make a more effective use of information in their decision processes. The shape of both curves of the MAB-MDP and Super-UCB is the same but, as previously anticipated, with a gap. The reason for this gap is the loss of information in the MAB-MDP algorithm.
% Figure \ref{fig:stoVsT} shows a comparison of our two proposals in the stochastic bandit scenario: the two-layer policy that addresses both anticipatory and experiential learning and the sequential superprocess with upper confidence bounds as rewards. 
% They are compared to the extension of the superprocess policy to experiential learning of SNRs, that is, re-computing the superprocess on each decision stage; and the $\epsilon$-greedy.  
% We consider that the algorithms know the standard deviation of the SNR of each arm, which is 5 dB \footnote{\cite{ref:Goldsmith2009} indicates typical values between 4-13 dB in empirical measurements.}. 

No surprise is also what happens with Super-0CB over time: the regret increases linearly on average due to the cases where the best arm showed bad initial samples and thus, a suboptimal arm is selected. 
It does perform better than the other algorithms in the initial frames simply because it does more exploitation than MAB-MDP and Super-UCB, and still balances exploration and exploitation of the offers of SUs unlike the $\epsilon$-greedy.

\subsubsection{Performance versus standard deviation}

Figure \ref{fig:VsKnownVar} shows the performance of the algorithms versus different values of $\sigma_{\gamma_s}$. The standard deviation for log-normal shadowing varies from 4 to 13 dB depending on the environment, and is related to the variations of the blockage that objects cause in the signal path \cite{ref:Goldsmith2005}. The higher the deviations, the harder the problem, as the samples of each distribution may not preserve the rank of their mean values, that is to say, bad arms may appear as the best ones and viceversa. The MAB-MDP and Super-UCB standard deviation terms are adjusted accordingly. 

$\epsilon$-greedy and Super-0CB algorithms experience notable performance degradation under high variance. This is because they rely more on the estimates of the mean reward. As the variance increases, more rounds are needed for the sample mean to converge to the true mean. Nevertheless, due to its random exploration nature, $\epsilon$-greedy manages to eventually find the optimal arm, which is something that cannot be said about Super-0CB and explains its poor results. 
The proposed UCB-based algorithms show certain robustness under high variances, since they take the standard deviation into account when making their decisions. 
 
\begin{figure}[!t]
\centering
\includegraphics{VsKnownVar.eps}
\caption{Regret at $n = 3000$ for the MAB-MDP and the $\epsilon$-greedy algorithms for different values of $\sigma_{\gamma_s}$. Mean SNRs and SU types are fixed over experiments. $\boldsymbol{\mu_{\gamma}}=\{35,32.5,32.5,30,30,27,27\}$ and SU types $= \{1,1,1,2,2,3,3\}$.}
\label{fig:VsKnownVar}
\end{figure}

\subsubsection{Performance under mis-estimation of standard deviation}

Fig. \ref{fig:unknownVar} shows the effect of mistestimating the standard deviation $\sigma_{\gamma_s}$ in our algorithms. $\epsilon$-greedy and Super-0CB perform a frequentist inference and therefore do not make use of the variance.

Underestimating or overestimating $\sigma_{\gamma_s}$ in the UCB index implies being less or more optimistic, respectively, about the values of the true average rewards of the SUs, based on the sample means observed. 
Thus, an overestimation implies more exploration of each SU and an increased probability of finding the optimal arm.
An underestimation implies more exploitation of SUs and less exploration. 
As shown in the figure, overestimation is safer than underestimation in terms of regret.
Overestimation causes a slower convergence towards the optimal arm but underestimation could make the algorithm converge to a suboptimal solution, which is associated to a linear regret with the number of rounds. The difference between the rewards of the optimal and of that suboptimal arm in each round accumulates over time.  
Note that, under severe underestimation, both MAB-MDP and Super-UCB can even do worse than the $\epsilon$-greedy algorithm, but no worse than Super-0CB
Underestimation may be justified in a scenario where the SUs are expected to stay a short time in the coverage area of the PT. 
Then, the PT may be better off sacrificing the search of an optimal arm for exploiting suboptimal arms with ``good enough'' rewards.
%It will have an impact in case of selecting a different prior belief (Bayesian inference): the variances of the initial beliefs about the true means and the assumed variances of the rewards act as ``weights'' when computing the mean. Underestimating the variance of the rewards will give less value than optimal to the empirical samples compared to the initial belief, and overestimating it will cause the opposite effect. The overall effect on the Bayesian inference would also depend on how accurate is the initial belief. 


\begin{figure}[!t]
\centering
\includegraphics{unknownVar.eps}
\caption{Regret at $n = 3000$ for different estimations of $\sigma_{\gamma_s}$, with the real value of $\sigma_{\gamma_s}$ = 10 dB. Mean SNRs and SU types are fixed over experiments.}
\label{fig:unknownVar}
\end{figure}

\subsubsection{Performance against number of SUs and discretization levels of the offers}

Figs. \ref{fig:VsN} and \ref{fig:VsA} illustrate how regret grows with respect to the optimal policy for the different algorithms when increasing the number of SUs in the system or the granularity of the offers the PT can make.
Note that a higher regret represents worse performance with respect to the optimal policy.
With a higher number of SUs or a higher number of possible offers, the ideal reference policy obtains better rewards. 
In short: finding the optimal arm is harder but that optimal arm provides a higher reward than the optimal arm of a scenario with fewer SUs or fewer possible offers. 
That said, both figures show that the proposed algorithms scale well when increasing the size of the problem, as their regret grows dramatically less than for the $\epsilon$-greedy or the basic Super-0CB algorithm. 
This is because of the more effective use of the available information made by the UCB-based algorithms. 
Moreover, the computational cost or memory requirements do not experience a significant growth, even in the worst case, which is increasing the number of offers. 
The state space of the MDP of an SU, $\mathcal{X}_s$, grows with the number of offers, $A$, as $|\mathcal{X}_s| = A(A+1)/2$, which for a fine-grained range of 20 possible offers becomes a tractable problem with 210 states and 20 actions in each state. 
\begin{figure}[!t]
\centering
\includegraphics{VsN.eps}
\caption{Regret at $n = 3000$ vs. the number of SUs, $S$. Number of possible offers $A = 7$. Again, for every arm $s$, $\sigma_{\gamma_s} = 5$ dB and $\mu_{\gamma_s}$ are chosen uniformly from the range $[5,40]$ dB.}
\label{fig:VsN}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics{VsA.eps}
\caption{Regret at $n = 3000$ vs. the number of possible offers, $A$. Number of SUs $S = 7$.}
\label{fig:VsA}
\end{figure}

\section{Conclusion and Future Work}\label{sec:Con}
%Brief summary
We have proposed a spectrum trading mechanism in Cooperative Spectrum Sharing (CSS) using multi-armed bandits (MABs), from the perspective of a primary transmitter (PT),  modeling channels under shadowing effects. 
We have focused on a scenario where the PT has no knowledge of the performance of the SUs acting as relays, or about the offers they are willing to accept. 
Our two algorithms, MAB-MDP and Super-UCB, have been shown to be able to learn payoff-maximizing actions for the PT with little communication or computation overhead. 
%Results
Our numerical results indicate that, despite their simplicity, they significantly outperform the classical exploration-exploitation $\epsilon$-greedy algorithm, with Super-UCB featuring a better overall performance.
They are shown to be robust to inaccuracies in the little information they need and to scale well when the size of the problem increases, \textit{i.e.,} for more SUs and available offers.
%Next
This work can be the starting point to develop more complex scenarios.  
Considering the explosion of MAB variants in the recent literature, as the next steps it would be possible and interesting to study: 1) how to exploit the spatial fading correlation across different SUs, 2) extension of the algorithms to a multiple PT and multiple PR case, 3) inclusion of more dimensions to learning, such as learning the staying time of SUs in the PT coverage area or the distribution of the SU types, and 4) inclusion of SU and PU strategic behaviors. 


